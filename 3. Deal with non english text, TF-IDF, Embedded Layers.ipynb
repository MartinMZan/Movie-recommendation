{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc29c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84475f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_raw(str):\n",
    "    # Remove punctuation\n",
    "    str = [char for char in str if char not in string.punctuation]\n",
    "    str = ''.join(str)\n",
    "    \n",
    "    # And use stemmer\n",
    "    str = str.split(' ')\n",
    "    str = [stemmer.stem(word) for word in str]\n",
    "    str = ' '.join(str)\n",
    "    \n",
    "    return str\n",
    "\n",
    "def remove_non_english(str):\n",
    "    str = str.split(' ')\n",
    "    str = [word for word in str if word not in englishwords]\n",
    "    str = ' '.join(str)\n",
    "    \n",
    "    return str\n",
    "\n",
    "def load_glove_model(File):\n",
    "    glove_model = {}\n",
    "    with open(File,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "    return glove_model\n",
    "\n",
    "def map_overview(line):\n",
    "    overviewvector = np.array([])\n",
    "    for word in line.split():\n",
    "        try:\n",
    "            overviewvector = np.concatenate([overviewvector, glove[word]], axis=0)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    if len(overviewvector) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Change data structure to [ [], [], [] ... ] format\n",
    "    ovec2 = np.zeros(shape=(int(len(overviewvector)/wordlength), wordlength))\n",
    "    for i in range( int(len(overviewvector)/wordlength) ) :\n",
    "        ovec2[i] = overviewvector[i * wordlength : i * wordlength + wordlength]\n",
    "        \n",
    "    # Calculate the center of the points\n",
    "    ovec2 = sum(ovec2)/len(ovec2)\n",
    "    \n",
    "    return ovec2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd765a93",
   "metadata": {},
   "source": [
    "1. part: Deal with non-english text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef738a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate movies to 2 files based on overview column\n",
    "movies = pd.read_csv('movies_metadata.csv', low_memory=False)\n",
    "\n",
    "movieswithoverview = movies[movies['overview'].notnull()]\n",
    "movies = movies[movies['overview'].isnull()]\n",
    "\n",
    "## Write\n",
    "# movies.to_csv('1a movies_withoutoverview.csv', index=False)\n",
    "# movieswithoverview.to_csv('1b movies_withoverview.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4faf4c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and select movies with overview\n",
    "movieswithoverview = pd.read_csv('1b movies_withoverview.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "459ce9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stemmer\n",
    "stemmer = EnglishStemmer()\n",
    "movieswithoverview['overview'] = movieswithoverview['overview'].apply(text_to_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faad738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of non english text\n",
    "# This took me at least 1 hour to run!\n",
    "englishwords = nltk.corpus.words.words()\n",
    "movieswithoverview['overview'] = movieswithoverview['overview'].apply(remove_non_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f614823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to file\n",
    "# movieswithoverview.dropna(inplace=True)\n",
    "# movieswithoverview.to_csv('2 movies_english.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bff391",
   "metadata": {},
   "source": [
    "2. part: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb2b3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "movieswithoverview = pd.read_csv('2 movies_english.csv', low_memory=False)\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "vector = tfidf.fit_transform(movieswithoverview['overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c6fb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<44062x56883 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 474015 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of words is reduced to ~57 000\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40a96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction\n",
    "k = 50 # Number of components\n",
    "svd = TruncatedSVD(n_components=k)\n",
    "vector = svd.fit_transform(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40934a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store vector into dataframe and join them\n",
    "components = pd.DataFrame(vector, columns=[str(i) + '. overview component' for i in range(0,k)])\n",
    "\n",
    "movieswithcomponents = movieswithoverview.join(components).drop('overview', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "745901e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to file\n",
    "movieswithcomponents.to_csv('3a movies_TFIDF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ce767",
   "metadata": {},
   "source": [
    "3. part: Embedded Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b8dda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "movieswithoverview = pd.read_csv('2 movies_english.csv', low_memory=False)\n",
    "\n",
    "glove = load_glove_model('glove.6B.50d.txt')\n",
    "wordlength = len(glove[list(glove.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "213ce8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedded Layers\n",
    "movieswithoverview['overview'] = movieswithoverview['overview'].apply(map_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5c0bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pd.DataFrame(movieswithoverview['overview'].to_list(), columns=[str(col) + '. overview component' for col in range(len(movieswithoverview['overview'][0]))])\n",
    "components = components.fillna(value=0)\n",
    "\n",
    "movieswithoverview = movieswithoverview.drop('overview', axis=1).join(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e39386b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to file\n",
    "# movieswithoverview.to_csv('3b movies_Embedded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22d0c4",
   "metadata": {},
   "source": [
    "Example how the text changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "326674e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zhigen, an old Chinese farmer, has lived alone in Beijing for over 20 years after moving to the city to allow his son Chongyi to attend university. He decides to make the long journey from Beijing to Yangshuo to honour the promise he made to his wife to bring back the bird that has been his only companion in the city. His daughter-in-law Qianying, a beautiful rich career woman, asks him to take along his granddaughter Renxing, an only child brought up in the lap of luxury. While grandfather and granddaughter set out on their journey - one travelling back in time, the other discovering her roots - Chongyi and Qianying, ponder the meaning of the life they have led in the sole pursuit of success and money.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieswithoverview['overview'][40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e51458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zhigen chines has alon beij 20 citi chongyi univers decid beij yangshuo honour promis has onli citi daughterinlaw qiani granddaught renx onli luxuri grandfath granddaught  discov  chongyi qiani'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_non_english(movieswithoverview['overview'][40000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
